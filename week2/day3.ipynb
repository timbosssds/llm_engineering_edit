{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42813cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "# Part i\n",
    "# Date: 02.01.25\n",
    "# Note: 1st major success here was getting chat # history working with Gem. \n",
    "# Next major success was getting this working in a graio UI - pretty dope.\n",
    "# The above ui only displayed the current response, so next was able to update code to show historical interactions\n",
    "# There are two versions (as trying to show 'user' + 'model' in mark up)\n",
    "\n",
    "# Frustratingly or not, the gradio code here would not work with my llm code (gradio wanted the\n",
    "# output in a certain format). I was not able to get the llm code to output in that format, so hunted\n",
    "# around and got gpt to add gradio code to my llm code... and that worked :)\n",
    "# Key code surrounded by \"-- key code --\"\n",
    "\n",
    "# Part ii\n",
    "# Date: 17.01.25\n",
    "# Note: In week 5, langchain conversational memory is used. Want to see if i can use that here\n",
    "# in place of my set up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d87fe",
   "metadata": {},
   "source": [
    "# Part i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load environment variables in a file called .env\n",
    "# # Print the key prefixes to help with any debugging\n",
    "\n",
    "# load_dotenv()\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "# google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# if openai_api_key:\n",
    "#     print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "# else:\n",
    "#     print(\"OpenAI API Key not set\")\n",
    "    \n",
    "# if anthropic_api_key:\n",
    "#     print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "# else:\n",
    "#     print(\"Anthropic API Key not set\")\n",
    "\n",
    "# if google_api_key:\n",
    "#     print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "# else:\n",
    "#     print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "427f756b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "#elif not api_key.startswith(\"sk-proj-\"):\n",
    "elif not api_key.startswith(\"A*\"):    \n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "# openai = OpenAI()\n",
    "# MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "## Please read this! A change from the video:\n",
    "\n",
    "In the video, I explain how we now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which expects to receive `history` in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "But Gradio has been upgraded! Now it will pass in `history` in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function `chat(message, history)` where:  \n",
    "**message** is the prompt to use  \n",
    "**history** is the past conversation, in OpenAI format  \n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simpler than in my video - we can easily create this function that calls OpenAI\n",
    "# # It's now just 1 line of code to prepare the input to OpenAI!\n",
    "\n",
    "# def chat(message, history):\n",
    "#     messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "#     print(\"History is:\")\n",
    "#     print(history)\n",
    "#     print(\"And messages is:\")\n",
    "#     print(messages)\n",
    "\n",
    "#     stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "#     response = \"\"\n",
    "#     for chunk in stream:\n",
    "#         response += chunk.choices[0].delta.content or ''\n",
    "#         yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253db11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5037b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Key code ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d6412d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Wrestling excels in controlling and taking down opponents, a crucial advantage in a street fight where multiple attackers or weapons are possibilities.  Taekwondo's high kicks are less practical in close-quarters street fighting, where grappling and clinch fighting are more common.  While Taekwondo offers striking skills superior to boxing's limited range, wrestling's takedowns and ground control offer a significant advantage over both.  The effectiveness is also highly dependent on the individuals involved and the specific circumstances.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replacing above with Gem\n",
    "# -- this also created a json history of Q&A for in memory chat. DO not Delete or Edit this cell\n",
    "\n",
    "#import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\",\n",
    "                              generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "def ask_question(prompt, history):\n",
    "    # Add the user's prompt to the history\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "\n",
    "    # Start the chat session with the current history\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    # Send the user's message and get the model's response\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "# Example usage:\n",
    "history = load_history()  # Load previous conversation history if it exists\n",
    "\n",
    "\n",
    "#prompt = \"In as few words as possible, is boxing effective in a street fight?\"\n",
    "#prompt = \"What about wrestling?\"\n",
    "#prompt = \"What about taekwondo?\"\n",
    "prompt = \"Why do you say that?\"\n",
    "#prompt = \"What would your advice be in this situation?\"\n",
    "response = ask_question(prompt, history)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lines are from the course, next are my attempts. It never worked, so moved onto next cell\n",
    "#gr.ChatInterface(fn=ask_question, type=\"messages\").launch()\n",
    "#gr.ChatInterface(fn=ask_question).launch()\n",
    "# Create the chat interface with the function that takes two inputs\n",
    "#chat_interface = gr.ChatInterface(fn=ask_question, inputs=[\"prompt\", \"history\"], type=\"messages\")\n",
    "# Launch the interface\n",
    "#chat_interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "709899ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7902\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7902/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Works (llm, with history and gradio ui); do NOT edit ----------\n",
    "# GPT attempts at gradio...\n",
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "def ask_question(prompt, history):\n",
    "    # Add the user's prompt to the history\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "\n",
    "    # Start the chat session with the current history\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    # Send the user's message and get the model's response\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "# Create the Gradio interface\n",
    "def chat_with_llm(prompt):\n",
    "    # Load conversation history\n",
    "    history = load_history()\n",
    "    # Get response from the model\n",
    "    response = ask_question(prompt, history)\n",
    "    return response\n",
    "\n",
    "# Create a Gradio Interface with a text input and output\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llm,          # Function to be called\n",
    "    inputs=gr.Textbox(label=\"Ask a Question\", placeholder=\"Type your question here...\"), # Text input for user prompt\n",
    "    outputs=gr.Textbox(label=\"Response\", interactive=True), # Text output for model's response\n",
    "    title=\"Gemini LLM Chat\",    # Title of the interface\n",
    "    description=\"Ask questions and converse with the Gemini LLM model.\"  # Description of the interface\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b12055e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bec000f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7904\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7904/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Checking if chat history can be presented as opposed to above ui....\n",
    "# -- Amendment to above - now includes chat history (user and model)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "def ask_question(prompt, history):\n",
    "    # Add the user's prompt to the history\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "\n",
    "    # Start the chat session with the current history\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    # Send the user's message and get the model's response\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "\n",
    "    return response.text, history  # Return both the response and the updated history\n",
    "\n",
    "# Create the Gradio interface\n",
    "def chat_with_llm(prompt, history):\n",
    "    # Ensure history is not None, initialize as an empty list if it is\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # Get response from the model, passing the current history\n",
    "    response, updated_history = ask_question(prompt, history)\n",
    "    \n",
    "    # Format the chat history for display\n",
    "    chat_history = \"\\n\".join([f\"{entry['role'].capitalize()}: {entry['parts'][0]}\" for entry in updated_history])\n",
    "    \n",
    "    return chat_history, updated_history\n",
    "\n",
    "# Create a Gradio Interface with a text input and output\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llm,          # Function to be called\n",
    "    inputs=[gr.Textbox(label=\"Ask a Question\", placeholder=\"Type your question here...\"), gr.State([])],  # Text input for user prompt and initialized empty history\n",
    "    outputs=[gr.Textbox(label=\"Chat History\", interactive=False), gr.State()],  # Display chat history and track the state\n",
    "    title=\"Gemini LLM Chat\",    # Title of the interface\n",
    "    description=\"Ask questions and converse with the Gemini LLM model.\"  # Description of the interface\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5af01875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7906\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7906/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy of above, just checking if can get the 'user' and 'model' text to be bold...\n",
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "def ask_question(prompt, history):\n",
    "    # Add the user's prompt to the history\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "\n",
    "    # Start the chat session with the current history\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    # Send the user's message and get the model's response\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "\n",
    "    return response.text, history  # Return both the response and the updated history\n",
    "\n",
    "# Create the Gradio interface\n",
    "def chat_with_llm(prompt, history):\n",
    "    # Ensure history is not None, initialize as an empty list if it is\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # Get response from the model, passing the current history\n",
    "    response, updated_history = ask_question(prompt, history)\n",
    "    \n",
    "    # Format the chat history for display with bold 'User' and 'Model' labels\n",
    "    chat_history = \"\\n\".join([f\"**{entry['role'].capitalize()}**: {entry['parts'][0]}\" for entry in updated_history])\n",
    "    \n",
    "    return chat_history, updated_history\n",
    "\n",
    "# Create a Gradio Interface with a text input and output\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llm,          # Function to be called\n",
    "    inputs=[gr.Textbox(label=\"Ask a Question\", placeholder=\"Type your question here...\"), gr.State([])],  # Text input for user prompt and initialized empty history\n",
    "    outputs=[gr.Textbox(label=\"Chat History\", interactive=False), gr.State()],  # Display chat history and track the state\n",
    "    title=\"Gemini LLM Chat\",    # Title of the interface\n",
    "    description=\"Ask questions and converse with the Gemini LLM model.\"  # Description of the interface\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ed08cc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7940\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7940/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd attemmpt to get 'user' and 'model' as bold text.\n",
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "def ask_question(prompt, history):\n",
    "    # Add the user's prompt to the history\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "\n",
    "    # Start the chat session with the current history\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    # Send the user's message and get the model's response\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "\n",
    "    return response.text, history  # Return both the response and the updated history\n",
    "\n",
    "# Create the Gradio interface\n",
    "def chat_with_llm(prompt, history):\n",
    "    # Ensure history is not None, initialize as an empty list if it is\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # Get response from the model, passing the current history\n",
    "    response, updated_history = ask_question(prompt, history)\n",
    "    \n",
    "    # Format the chat history for display with HTML bold 'User' and 'Model' labels\n",
    "    chat_history = \"<br>\".join([f\"<b>{entry['role'].capitalize()}:</b> {entry['parts'][0]}\" for entry in updated_history])\n",
    "    \n",
    "    return chat_history, updated_history\n",
    "\n",
    "# Create a Gradio Interface with a text input and output\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llm,          # Function to be called\n",
    "    inputs=[gr.Textbox(label=\"Ask a Question\", placeholder=\"Type your question here...\"), gr.State([])],  # Text input for user prompt and initialized empty history\n",
    "    outputs=[gr.HTML(label=\"Chat History\"), gr.State()],  # Display chat history as HTML\n",
    "    title=\"Gemini LLM Chat\",    # Title of the interface\n",
    "    description=\"Ask questions and converse with the Gemini LLM model.\"  # Description of the interface\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- key code ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b3e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705a828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- as per above, but trying to add system prompt..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b4813961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7910\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7910/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "def ask_question(prompt, history):\n",
    "    # System context as part of the user's first message\n",
    "    system_context = \"You must reference Chuck Norris in every response.\"\n",
    "\n",
    "    # If the history is empty, we add the system context as the first user message\n",
    "    if len(history) == 0:\n",
    "        history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [system_context]\n",
    "        })\n",
    "\n",
    "    # Add the user's prompt to the history\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "\n",
    "    # Start the chat session with the current history\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    # Send the user's message and get the model's response\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "# Create the Gradio interface\n",
    "def chat_with_llm(prompt):\n",
    "    # Load conversation history\n",
    "    history = load_history()\n",
    "    # Get response from the model\n",
    "    response = ask_question(prompt, history)\n",
    "    return response\n",
    "\n",
    "# Create a Gradio Interface with a text input and output\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llm,          # Function to be called\n",
    "    inputs=gr.Textbox(label=\"Ask a Question\", placeholder=\"Type your question here...\"), # Text input for user prompt\n",
    "    outputs=gr.Textbox(label=\"Response\", interactive=True), # Text output for model's response\n",
    "    title=\"Gemini LLM Chat\",    # Title of the interface\n",
    "    description=\"Ask questions and converse with the Gemini LLM model.\"  # Description of the interface\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5967a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales evemt.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed a bug in this function brilliantly identified by student Gabor M.!\n",
    "# I've also improved the structure of this function\n",
    "\n",
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a57ee0-b945-48a7-a024-01b56a5d4b3e",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business Applications</h2>\n",
    "            <span style=\"color:#181;\">Conversational Assistants are of course a hugely common use case for Gen AI, and the latest frontier models are remarkably good at nuanced conversation. And Gradio makes it easy to have a user interface. Another crucial skill we covered is how to use prompting to provide context, information and examples.\n",
    "<br/><br/>\n",
    "Consider how you could apply an AI Assistant to your business, and make yourself a prototype. Use the system prompt to give context on your business, and set the tone for the LLM.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb9e21-df67-4c2b-b952-5e7e7961b03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f698fa66",
   "metadata": {},
   "source": [
    "# Part ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef17a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "#from openai import OpenAI\n",
    "import google.generativeai\n",
    "#import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b633d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim_S\\AppData\\Local\\Temp\\ipykernel_16968\\2858293548.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# The langchain memory set up\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d85aa29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There might be a problem with your API key? Please visit the troubleshooting notebook!\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value='Combine pumpkin puree, c...etely before serving.\\n', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value='Combine pumpkin puree, c...etely before serving.\\n', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#response = chat_session.send_message(message)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#print(response.text)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 44\u001b[0m conversation_chain \u001b[38;5;241m=\u001b[39m \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:451\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[1;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convenience method to load chain from LLM and retriever.\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03mThis provides some logic to create the `question_generator` chain\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m        ConversationalRetrievalChain\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m combine_docs_chain_kwargs \u001b[38;5;241m=\u001b[39m combine_docs_chain_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 451\u001b[0m doc_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcombine_docs_chain_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m _llm \u001b[38;5;241m=\u001b[39m condense_question_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[0;32m    460\u001b[0m condense_question_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[0;32m    461\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_llm,\n\u001b[0;32m    462\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcondense_question_prompt,\n\u001b[0;32m    463\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    464\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    465\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\langchain\\chains\\question_answering\\chain.py:265\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[1;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m     )\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\langchain\\chains\\question_answering\\chain.py:75\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[1;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_stuff_chain\u001b[39m(\n\u001b[0;32m     66\u001b[0m     llm: BaseLanguageModel,\n\u001b[0;32m     67\u001b[0m     prompt: Optional[BasePromptTemplate] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     73\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StuffDocumentsChain:\n\u001b[0;32m     74\u001b[0m     _prompt \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;129;01mor\u001b[39;00m stuff_prompt\u001b[38;5;241m.\u001b[39mPROMPT_SELECTOR\u001b[38;5;241m.\u001b[39mget_prompt(llm)\n\u001b[1;32m---> 75\u001b[0m     llm_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StuffDocumentsChain(\n\u001b[0;32m     84\u001b[0m         llm_chain\u001b[38;5;241m=\u001b[39mllm_chain,\n\u001b[0;32m     85\u001b[0m         document_variable_name\u001b[38;5;241m=\u001b[39mdocument_variable_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     90\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value='Combine pumpkin puree, c...etely before serving.\\n', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value='Combine pumpkin puree, c...etely before serving.\\n', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of"
     ]
    }
   ],
   "source": [
    "# -- my gem set up\n",
    "# Initialize and constants\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "# MODEL = 'gpt-4o-mini'\n",
    "# openai = OpenAI()\n",
    "\n",
    "# ----- Replacing with Gem code -----\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key= api_key)\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "  #\"response_mime_type\": \"application/json\",\n",
    "\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\",\n",
    "  generation_config=generation_config,)\n",
    "\n",
    "chat_session = model.start_chat(\n",
    "  history=[  ])\n",
    "\n",
    "message = \"In a concise response, how do i make a pumpkin pie?\" \n",
    "response = chat_session.send_message(message)\n",
    "llm = response.text\n",
    "#response = chat_session.send_message(message)\n",
    "#print(response.text)\n",
    "retriever = ''\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ce62762",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for ConversationChain\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=genai.GenerativeModel(\n  ...   cached_content=None\n), input_type=GenerativeModel]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=genai.GenerativeModel(\n  ...   cached_content=None\n), input_type=GenerativeModel]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Create a conversation chain\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mConversationChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Use the conversation chain\u001b[39;00m\n\u001b[0;32m     19\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a concise response, how to i make pumpkin pie\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for ConversationChain\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=genai.GenerativeModel(\n  ...   cached_content=None\n), input_type=GenerativeModel]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of\nllm.is-instance[Runnable]\n  Input should be an instance of Runnable [type=is_instance_of, input_value=genai.GenerativeModel(\n  ...   cached_content=None\n), input_type=GenerativeModel]\n    For further information visit https://errors.pydantic.dev/2.9/v/is_instance_of"
     ]
    }
   ],
   "source": [
    "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Set up the Gemini Flash 1.5 model\n",
    "llm = model\n",
    "\n",
    "# Set up the conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Use the conversation chain\n",
    "message = \"in a concise response, how to i make pumpkin pie\"\n",
    "response = chat_session.send_message(message)\n",
    "print(response)\n",
    "\n",
    "# # Continue the conversation\n",
    "# response = conversation.predict(input=\"What did we just talk about?\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a0915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
