{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfa9ae6-69fe-444a-b994-8c4c5970a7ec",
   "metadata": {},
   "source": [
    "# Project - Airline AI Assistant\n",
    "\n",
    "We'll now bring together what we've learned to make an AI Customer Support assistant for an Airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3faacd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 04.01.25\n",
    "# Note: Not using the openai approach, but...\n",
    "# Big success in getting Flash to make function calls. \n",
    "# Then copying idea from day 3 was able to use GPT to get the Flash function calling woking with Gradio!!!\n",
    "# Then in hacky way improved above, so it doesn't call the 'get_price_function' every time it sees a city,\n",
    "# it now also looks for related key words (ticket, cost, etc.), so if the question is unrelated to travel it can \n",
    "# answer without spitting out ticket price.\n",
    "# I will highlight that code with # -- key code -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e71bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8b50bbe2-c0b1-49c3-9a5c-1ba7efa2bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "#from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "747e8786-9da8-4342-b6c9-f5f69c2e22ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\n"
     ]
    }
   ],
   "source": [
    "# # Load environment variables in a file called .env\n",
    "\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# # Check the key\n",
    "\n",
    "# if not api_key:\n",
    "#     print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "# #elif not api_key.startswith(\"sk-proj-\"):\n",
    "# elif not api_key.startswith(\"A*\"):    \n",
    "#     print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "# elif api_key.strip() != api_key:\n",
    "#     print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "# else:\n",
    "#     print(\"API key found and looks good so far!\")\n",
    "\n",
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "#elif not api_key.startswith(\"sk-proj-\"):\n",
    "elif not api_key.startswith(\"A*\"):    \n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0a521d84-d07c-49ab-a0df-d6451499ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a3264e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Key code ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "61a2a15d-b559-4844-b377-6bd5cb4949f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool get_ticket_price called for london\n",
      "The ticket price for London is $799.\n"
     ]
    }
   ],
   "source": [
    "# Ignore provided code as not using open ai\n",
    "# -- Was able to use Flash and make function calls using the theme of this notebook.\n",
    "# Do NOT edit\n",
    "\n",
    "# This function looks rather simpler than the one from my video, because we're taking advantage of the latest Gradio updates\n",
    "\n",
    "# def chat(message, history):\n",
    "#     messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "#     response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# gr.ChatInterface(fn=chat, type=\"messages\").launch()\n",
    "\n",
    "\n",
    "# Replacing above with Gem\n",
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\",\n",
    "                              generation_config=generation_config,)\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "\n",
    "# --- Your ticket price function (Tool) ---\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")\n",
    "\n",
    "# Function to handle a chat with the Generative AI model\n",
    "def ask_chat_model(prompt, history):\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "    return response.text\n",
    "\n",
    "# Function to check if a question is about ticket prices and respond accordingly\n",
    "def handle_ticket_price_request(prompt):\n",
    "    # Check if the prompt is related to a ticket price request (contains a city)\n",
    "    cities = list(ticket_prices.keys())\n",
    "    for city in cities:\n",
    "        if city.lower() in prompt.lower():\n",
    "            # If it's a ticket price request, call the tool function\n",
    "            price = get_ticket_price(city)\n",
    "            return f\"The ticket price for {city.capitalize()} is {price}.\"\n",
    "    return None\n",
    "\n",
    "# Main function to ask a question and determine if it's a ticket request or a chat request\n",
    "def ask_question(prompt, history):\n",
    "    # First, check if the prompt is related to a ticket price\n",
    "    ticket_response = handle_ticket_price_request(prompt)\n",
    "    if ticket_response:\n",
    "        # If it's a ticket price request, return the response\n",
    "        return ticket_response\n",
    "    # Otherwise, proceed with the chat model if it's not about a ticket price\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "    return ask_chat_model(prompt, history)\n",
    "\n",
    "# Example usage:\n",
    "history = load_history()  # Load previous conversation history if it exists\n",
    "prompt = \"How much is the ticket to London?\"\n",
    "#prompt = \"How much is the ticket to Spain?\"\n",
    "#prompt = \"Is boxing effective in a street fight?\"\n",
    "#prompt = \"What about karate?\"\n",
    "#prompt = \"How much is a trip to Paris?\"\n",
    "response = ask_question(prompt, history)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5dde63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds above capability to Gradio! (Could try to make this code more consistent with day 3 if desired, but this works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "050ad633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7920\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7920/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy of above (i.e. Flash with function call, but also adds Gradio UI)\n",
    "# -- Do NOT edit ---\n",
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "# --- Your ticket price function (Tool) ---\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")\n",
    "\n",
    "# Function to handle a chat with the Generative AI model\n",
    "def ask_chat_model(prompt, history):\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "    return response.text\n",
    "\n",
    "# Function to check if a question is about ticket prices and respond accordingly\n",
    "def handle_ticket_price_request(prompt):\n",
    "    # Check if the prompt is related to a ticket price request (contains a city)\n",
    "    cities = list(ticket_prices.keys())\n",
    "    for city in cities:\n",
    "        if city.lower() in prompt.lower():\n",
    "            # If it's a ticket price request, call the tool function\n",
    "            price = get_ticket_price(city)\n",
    "            return f\"The ticket price for {city.capitalize()} is {price}.\"\n",
    "    return None\n",
    "\n",
    "# Main function to ask a question and determine if it's a ticket request or a chat request\n",
    "def ask_question(prompt, history):\n",
    "    # First, check if the prompt is related to a ticket price\n",
    "    ticket_response = handle_ticket_price_request(prompt)\n",
    "    if ticket_response:\n",
    "        # If it's a ticket price request, return the response\n",
    "        return ticket_response #, history\n",
    "    # Otherwise, proceed with the chat model if it's not about a ticket price\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "    response = ask_chat_model(prompt, history)\n",
    "    #return response, history\n",
    "    return response\n",
    "\n",
    "# Function to handle the Gradio interface\n",
    "def gradio_chat(message, history):\n",
    "    # Process the message, handling both ticket and chat requests\n",
    "    response, updated_history = ask_question(message, history)\n",
    "    #return response, updated_history\n",
    "    return response\n",
    "\n",
    "# Create the Gradio interface\n",
    "def chat_with_llm(prompt):\n",
    "    # Load conversation history\n",
    "    history = load_history()\n",
    "    # Get response from the model\n",
    "    response = ask_question(prompt, history)\n",
    "    return response\n",
    "\n",
    "# Create a Gradio Interface with a text input and output\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llm,          # Function to be called\n",
    "    inputs=gr.Textbox(label=\"Ask a Question\", placeholder=\"Type your question here...\"), # Text input for user prompt\n",
    "    outputs=gr.Textbox(label=\"Response\", interactive=True), # Text output for model's response\n",
    "    title=\"Gemini LLM Chat\",    # Title of the interface\n",
    "    description=\"Ask questions and converse with the Gemini LLM model.\"  # Description of the interface\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2db7f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above uses key word to trigger the function, i.e. if text contains 'london' then trigger ticket function. But what if customer is asking\n",
    "# how to get to london, not whats the ticket price. Below tries to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2185076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7921\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7921/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Very hacky. It now looks for the city and key words related to travel.\n",
    "# i.e. if it sees a city, it then looks for travel related words also, before calling function.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "# --- Your ticket price function (Tool) ---\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")\n",
    "\n",
    "# Function to check if the question is related to ticket prices (based on keywords)\n",
    "def is_ticket_price_request(prompt):\n",
    "    # List of cities to check for ticket price mention\n",
    "    cities = list(ticket_prices.keys())\n",
    "    \n",
    "    # Check if the prompt contains a city and a price-related question\n",
    "    for city in cities:\n",
    "        if city.lower() in prompt.lower():\n",
    "            # Further check if the user is asking for ticket prices (avoid questions like \"how to get to\")\n",
    "            if \"ticket\" in prompt.lower() or \"price\" in prompt.lower() \\\n",
    "            or \"travel\" in prompt.lower() or \"cost\" in prompt.lower():\n",
    "                return True\n",
    "    return False\n",
    " \n",
    "# Function to handle a chat with the Generative AI model\n",
    "def ask_chat_model(prompt, history):\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "    return response.text\n",
    "\n",
    "# Main function to ask a question and determine if it's a ticket request or a chat request\n",
    "def ask_question(prompt, history):\n",
    "    # Check if the prompt is related to a ticket price request\n",
    "    if is_ticket_price_request(prompt):\n",
    "        # Extract the city name from the question if it's a ticket price request\n",
    "        cities = list(ticket_prices.keys())\n",
    "        for city in cities:\n",
    "            if city.lower() in prompt.lower():\n",
    "                # If it's a ticket price request, call the tool function\n",
    "                price = get_ticket_price(city)\n",
    "                return f\"The ticket price for {city.capitalize()} is {price}.\"\n",
    "    \n",
    "    # If it's not a ticket price request, proceed with the chat model\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "    return ask_chat_model(prompt, history)\n",
    "\n",
    "# Function to handle the Gradio interface\n",
    "def gradio_chat(message, history):\n",
    "    # Process the message, handling both ticket and chat requests\n",
    "    response = ask_question(message, history)\n",
    "    return response\n",
    "\n",
    "# Create the Gradio interface\n",
    "def chat_with_llm(prompt):\n",
    "    # Load conversation history\n",
    "    history = load_history()\n",
    "    # Get response from the model\n",
    "    response = ask_question(prompt, history)\n",
    "    return response\n",
    "\n",
    "# Create a Gradio Interface with a text input and output\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llm,          # Function to be called\n",
    "    inputs=gr.Textbox(label=\"Ask a Question\", placeholder=\"Type your question here...\"), # Text input for user prompt\n",
    "    outputs=gr.Textbox(label=\"Response\", interactive=True), # Text output for model's response\n",
    "    title=\"Gemini LLM Chat\",    # Title of the interface\n",
    "    description=\"Ask questions and converse with the Gemini LLM model.\"  # Description of the interface\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a4a8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Key code ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baceb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bda054a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7929\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7929/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'y\\n' in the string, so is price related!\n",
      "prompt: How much to get to Berlin?\n",
      "Tool get_ticket_price called for berlin\n"
     ]
    }
   ],
   "source": [
    "# Copy of above - but trying swap out key word search for llm call when checking if question relates to travel\n",
    "# Very hacky. It now looks for the city and key words related to travel.\n",
    "# i.e. if it sees a city, it then looks for travel related words also, before calling function.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", generation_config=generation_config)\n",
    "\n",
    "# Path to save the history file\n",
    "history_file = \"conversation_history.json\"\n",
    "\n",
    "# Function to load history from file\n",
    "def load_history():\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to save history to file\n",
    "def save_history(history):\n",
    "    with open(history_file, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "# --- Your ticket price function (Tool) ---\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")\n",
    "\n",
    "\n",
    "# ---------- below is the edit ----------\n",
    "\n",
    "# # Function to check if the question is related to ticket prices (based on keywords)\n",
    "def is_ticket_price_request(prompt):\n",
    "    # List of cities to check for ticket price mention\n",
    "    cities = list(ticket_prices.keys())\n",
    "    \n",
    "    # Check if the prompt contains a city and a price-related question\n",
    "    for city in cities:\n",
    "        if city.lower() in prompt.lower():\n",
    "            # Further check if the user is asking for ticket prices (avoid questions like \"how to get to\")\n",
    "            if \"ticket\" in prompt.lower() or \"price\" in prompt.lower() \\\n",
    "            or \"travel\" in prompt.lower() or \"cost\" in prompt.lower():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to check if the question is related to ticket prices (based on llm)\n",
    "def is_ticket_price_request(prompt):\n",
    "    # List of cities to check for ticket price mention\n",
    "    cities = list(ticket_prices.keys())\n",
    "    system = f\"\"\"Please review {prompt} to check if it is specifically ticket related, i.e \n",
    "            Question includes things like - how much, cost, price, ticket, etc. then respond with 'y' \n",
    "            if {prompt} is general in nature, i.e\n",
    "            Question includes things like - is it a nice place, how to get to, where is then respond with 'n'\"\"\"\n",
    "    chat_session = model.start_chat(history=[  ])\n",
    "    response = chat_session.send_message(system)\n",
    "    response.text\n",
    "    for city in cities:\n",
    "        if city.lower() in prompt.lower():\n",
    "            if 'y\\n' in response.text:\n",
    "                # Do something if 'n\\n' is found in the string\n",
    "                print(\"Found 'y\\\\n' in the string, so is price related!\")\n",
    "                print('prompt:', prompt)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "    # system = f\"\"\"if {prompt} is specifically related ticket price, i.e \n",
    "    #         Question relates to price, ticket, cost, etc.; respond with 'y' \n",
    "    #         if {prompt} is general in nature, i.e\n",
    "    #         Question relates to, how do i get to, where is, etc; respond with 'n'\"\"\"\n",
    "\n",
    "\n",
    "# Function to check if the question is related to ticket prices (based on keywords)\n",
    "# def is_ticket_price_request(prompt):\n",
    "#     system = f\"\"\"if {prompt} is related ticket price, for example when a customer asks 'How much is a ticket to this city'; respond with 'y' else respond with 'n'\"\"\"\n",
    "#     chat_session = model.start_chat(history=[  ])\n",
    "#     response = chat_session.send_message(system)\n",
    "#     response.text\n",
    "\n",
    "#     if 'n\\n' in response.text:\n",
    "#     # Do something if 'n\\n' is found in the string\n",
    "#          print(\"Found 'n\\\\n' in the string, so not price related!\")\n",
    "#          return True\n",
    "#     # else:\n",
    "#     # # Do something if 'n\\n' is not found\n",
    "#     #      print(\"Did not find 'y\\\\n' in the string, so NOT price related.\")\n",
    "#     #      return False\n",
    "\n",
    "# ---------- above is the edit ----------\n",
    "\n",
    "\n",
    " \n",
    "# Function to handle a chat with the Generative AI model\n",
    "def ask_chat_model(prompt, history):\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    response = chat_session.send_message(prompt)\n",
    "    # Add the model's response to the history\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [response.text]\n",
    "    })\n",
    "    # Save the updated history to file\n",
    "    save_history(history)\n",
    "    return response.text\n",
    "\n",
    "# Main function to ask a question and determine if it's a ticket request or a chat request\n",
    "def ask_question(prompt, history):\n",
    "    # Check if the prompt is related to a ticket price request\n",
    "    if is_ticket_price_request(prompt):\n",
    "        # Extract the city name from the question if it's a ticket price request\n",
    "        cities = list(ticket_prices.keys())\n",
    "        for city in cities:\n",
    "            if city.lower() in prompt.lower():\n",
    "                # If it's a ticket price request, call the tool function\n",
    "                price = get_ticket_price(city)\n",
    "                return f\"The ticket price for {city.capitalize()} is {price}.\"\n",
    "    \n",
    "    # If it's not a ticket price request, proceed with the chat model\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [prompt]\n",
    "    })\n",
    "    return ask_chat_model(prompt, history)\n",
    "\n",
    "# Function to handle the Gradio interface\n",
    "def gradio_chat(message, history):\n",
    "    # Process the message, handling both ticket and chat requests\n",
    "    response = ask_question(message, history)\n",
    "    return response\n",
    "\n",
    "# Create the Gradio interface\n",
    "def chat_with_llm(prompt):\n",
    "    # Load conversation history\n",
    "    history = load_history()\n",
    "    # Get response from the model\n",
    "    response = ask_question(prompt, history)\n",
    "    return response\n",
    "\n",
    "# Create a Gradio Interface with a text input and output\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llm,          # Function to be called\n",
    "    inputs=gr.Textbox(label=\"Ask a Question\", placeholder=\"Type your question here...\"), # Text input for user prompt\n",
    "    outputs=gr.Textbox(label=\"Response\", interactive=True), # Text output for model's response\n",
    "    title=\"Gemini LLM Chat\",    # Title of the interface\n",
    "    description=\"Ask questions and converse with the Gemini LLM model.\"  # Description of the interface\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a418c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'n\\n' in the string!\n"
     ]
    }
   ],
   "source": [
    "#prompt = \"how much for a ticket to london?\"\n",
    "#prompt = \"What to kittens eat\"\n",
    "prompt = \"how do i get to Paris?\"\n",
    "system = f\"\"\"if {prompt} is specifically related ticket price, i.e \n",
    "            if a customer asks 'How much is a ticket to this city'; respond with 'y' \n",
    "            if the {prompt} is general in nature, i.e\n",
    "            'how do i get to this city; respond with 'n'\"\"\"\n",
    "chat_session = model.start_chat(history=[  ])\n",
    "response = chat_session.send_message(system)\n",
    "response.text\n",
    "\n",
    "if 'n\\n' in response.text:\n",
    "    # Do something if 'n\\n' is found in the string\n",
    "    print(\"Found 'n\\\\n' in the string!\")\n",
    "else:\n",
    "    # Do something if 'y\\n' is not found\n",
    "    print(\"Found 'y\\\\n' in the string.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    chat_session = model.start_chat(\n",
    "    history=[  ])\n",
    "\n",
    "    response = chat_session.send_message(prompt)\n",
    "    #print(response.text)\n",
    "    return response.text\n",
    "prompt = \"What is today's date?\"\n",
    "message_gpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791803cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36bedabf-a0a7-4985-ad8e-07ed6a55a3a4",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Tools are an incredibly powerful feature provided by the frontier LLMs.\n",
    "\n",
    "With tools, you can write a function, and have the LLM call that function as part of its response.\n",
    "\n",
    "Sounds almost spooky.. we're giving it the power to run code on our machine?\n",
    "\n",
    "Well, kinda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0696acb1-0b05-4dc2-80d5-771be04f1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80ca4e09-6287-4d3f-997d-fa6afbcf6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool get_ticket_price called for Berlin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'$499'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ticket_price(\"Berlin\")\n",
    "#get_ticket_price(\"Texas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4afceded-7178-4c05-8fa6-9f2085e6a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a particular dictionary structure that's required to describe our function:\n",
    "\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\",\n",
    "    \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdca8679-935f-4e7f-97e6-e71a4d4f228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this is included in a list of tools:\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": price_function}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3554f-b4e3-4ce7-af6f-68faa6dd2340",
   "metadata": {},
   "source": [
    "## Getting OpenAI to use our Tool\n",
    "\n",
    "There's some fiddly stuff to allow OpenAI \"to call our tool\"\n",
    "\n",
    "What we actually do is give the LLM the opportunity to inform us that it wants us to run the tool.\n",
    "\n",
    "Here's how the new chat function looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b0744-9c78-408d-b9df-9f6fd9ed78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Provided code: Not using as uses open AI\n",
    "# def chat(message, history):\n",
    "#     messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "#     response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "\n",
    "#     if response.choices[0].finish_reason==\"tool_calls\":\n",
    "#         message = response.choices[0].message\n",
    "#         response, city = handle_tool_call(message)\n",
    "#         messages.append(message)\n",
    "#         messages.append(response)\n",
    "#         response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    \n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# # We have to write that function handle_tool_call:\n",
    "\n",
    "# def handle_tool_call(message):\n",
    "#     tool_call = message.tool_calls[0]\n",
    "#     arguments = json.loads(tool_call.function.arguments)\n",
    "#     city = arguments.get('destination_city')\n",
    "#     price = get_ticket_price(city)\n",
    "#     response = {\n",
    "#         \"role\": \"tool\",\n",
    "#         \"content\": json.dumps({\"destination_city\": city,\"price\": price}),\n",
    "#         \"tool_call_id\": tool_call.id\n",
    "#     }\n",
    "#     return response, city\n",
    "\n",
    "# gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0992986-ea09-4912-a076-8e5603ee631f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1592, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 836, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 618, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Tim_S\\Desktop\\bt\\AIEng\\llm_engineering\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Tim_S\\AppData\\Local\\Temp\\ipykernel_22268\\3315540042.py\", line 5, in chat\n",
      "    response = ask_question(prompt, history, tools=tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ask_question() got an unexpected keyword argument 'tools'\n"
     ]
    }
   ],
   "source": [
    "# Provided code: Not using as uses open AI\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    #response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "    response = ask_question(prompt, history, tools=tools)\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response, city = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        #response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "        response = ask_question(prompt, history)\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    city = arguments.get('destination_city')\n",
    "    price = get_ticket_price(city)\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"destination_city\": city,\"price\": price}),\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    return response, city\n",
    "\n",
    "\n",
    "#gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be8a71-b19e-4c2f-80df-f59ff2661f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
