{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 16.01.25\n",
    "# Note: I couldn't get the Flash version running as not sure how to set up the langchain, \n",
    "# conversational agent with Flash. Found this in the community contibutions. It works\n",
    "# using Lamma (and local embedding model - i replaced the one in here)\n",
    "# Need to double check to verify, it is using local Llama (i think it must be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Document loading, retrieval methods and text splitting\n",
    "# !pip install -qU langchain langchain_community\n",
    "\n",
    "# # Local vector store via Chroma\n",
    "# !pip install -qU langchain_chroma\n",
    "\n",
    "# # Local inference and embeddings via Ollama\n",
    "!pip install -qU langchain_ollama\n",
    "\n",
    "# # Web Loader\n",
    "# !pip install -qU beautifulsoup4\n",
    "\n",
    "# # Pull the model first\n",
    "# !ollama pull nomic-embed-text\n",
    "\n",
    "# !pip install -qU pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "#from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim_S\\AppData\\Local\\Temp\\ipykernel_18452\\4041424186.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 123 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "DB_NAME = \"vector_db\"\n",
    "\n",
    "#embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(DB_NAME):\n",
    "    Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=DB_NAME)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run a quick test - should return a list of documents = 4\n",
    "question = \"What kind of grill is the Spirt II?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_type': 'employees', 'source': 'knowledge-base\\\\employees\\\\Alex Harper.md'}, page_content='## Annual Performance History  \\n- **2021**:  \\n  - **Performance Rating**: 4.5/5  \\n  - **Key Achievements**: Exceeded lead generation targets by 30%. Introduced a new CRM analytics tool resulting in improved tracking of customer interactions.  \\n\\n- **2022**:  \\n  - **Performance Rating**: 4.8/5  \\n  - **Key Achievements**: Awarded \"SDR of the Year\" for outstanding contributions. Instrumental in securing 15 new B2B contracts, surpassing targets by 40%.  \\n\\n- **2023**:  \\n  - **Performance Rating**: 4.7/5  \\n  - **Key Achievements**: Played a key role in the launch of a new product line with a 25% increase in lead-to-conversion rates. Completed advanced sales negotiation training with high marks.  \\n\\n## Compensation History  \\n- **2021**:  \\n  - **Base Salary**: $55,000  \\n  - **Bonus**: $5,500 (10% of base due to performance)  \\n\\n- **2022**:  \\n  - **Base Salary**: $65,000 (Promotion to Senior SDR)  \\n  - **Bonus**: $13,000 (20% of base due to performance)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is from Week 1, day 2\n",
    "# This should guarantee it is using my local llama\n",
    "# Constants\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\" # Default\n",
    "#MODEL = \"llama3.2:1b\" # Trying as might be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim_S\\AppData\\Local\\Temp\\ipykernel_18452\\1110270455.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# create a new Chat with Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "#MODEL = \"llama3.2:latest\" # This was in the code\n",
    "MODEL = MODEL # replacing above with above cell\n",
    "llm = ChatOllama(temperature=0.7, model=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='How do i make a pumpkin pie?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know the specific details about making a pumpkin pie using Markellm's platform and services, as that information was only mentioned in the contract with Belvedere Insurance for Markellm. However, I can provide you with a general recipe and instructions on how to make a classic pumpkin pie:\\n\\nIngredients:\\n\\n* 1 cup of pumpkin puree\\n* 1 1/2 cups of heavy cream\\n* 1/2 cup of granulated sugar\\n* 1/2 teaspoon of salt\\n* 1/2 teaspoon of ground cinnamon\\n* 1/4 teaspoon of ground nutmeg\\n* 1/4 teaspoon of ground ginger\\n* 2 large eggs\\n\\nInstructions:\\n\\n1. Preheat your oven to 425°F (220°C).\\n2. In a large bowl, whisk together the pumpkin puree, heavy cream, sugar, salt, cinnamon, nutmeg, and ginger until well combined.\\n3. Beat in the eggs until smooth.\\n4. Roll out a pie crust and place it in a 9-inch pie dish.\\n5. Pour the pumpkin mixture into the pie crust.\\n6. Bake for 15 minutes at 425°F (220°C), then reduce the heat to 350°F (180°C) and bake for an additional 30-40 minutes, or until the filling is set and the crust is golden brown.\\n7. Allow the pie to cool before serving.\\n\\nNote: You can use a pre-made pie crust or make your own using a recipe like this one:\\n\\nIngredients:\\n\\n* 2 cups of all-purpose flour\\n* 1 teaspoon of salt\\n* 1/2 cup of cold unsalted butter, cut into small pieces\\n* 1/4 cup of ice water\\n\\nInstructions:\\n\\n1. In a large bowl, whisk together the flour and salt.\\n2. Add the cold butter and use a pastry blender or your fingers to work it into the flour until the mixture resembles coarse crumbs.\\n3. Gradually add the ice water, stirring with a fork until the dough comes together in a ball.\\n4. Wrap the dough in plastic wrap and refrigerate for at least 30 minutes before rolling out.\\n\\nI hope this helps! If you have any specific questions or need further clarification, feel free to ask.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What about Cherry?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know, but I can try to help you find some information on making a cherry pie! Would you like a general recipe or some specific guidance on a particular step of the process?\\n\\nIf you're looking for a classic cherry pie recipe, here's a simple one:\\n\\nIngredients:\\n\\nFor the Crust:\\n\\n* 2 cups all-purpose flour\\n* 1 teaspoon salt\\n* 1/2 cup cold unsalted butter, cut into small pieces\\n* 1/4 cup shortening (such as Crisco), chilled and cut into small pieces\\n\\nFor the Filling:\\n\\n* 2 cups fresh or frozen cherries, pitted\\n* 1/3 cup granulated sugar\\n* 2 tablespoons cornstarch\\n* 1 tablespoon lemon juice\\n* 1/4 teaspoon ground cinnamon\\n* 1/4 teaspoon ground nutmeg\\n* 1/4 teaspoon salt\\n\\nInstructions:\\n\\n1. Preheat your oven to 375°F (190°C).\\n2. Make the crust: In a large bowl, combine flour and salt. Add cold butter and shortening, using a pastry blender or your fingers to work them into the flour until it resembles coarse crumbs.\\n3. Gradually add ice-cold water, stirring with a fork until the dough comes together in a ball. Divide the dough in half, shape each half into a disk, and wrap in plastic wrap. Refrigerate for at least 30 minutes.\\n4. Make the filling: In a separate bowl, combine cherries, sugar, cornstarch, lemon juice, cinnamon, nutmeg, and salt. Let it sit for about 15 minutes, until the cherries start to release their juice.\\n5. Roll out one of the chilled dough disks to fit a 9-inch pie dish. Place the dough in the pie dish and trim the edges.\\n6. Fill the pie crust with the cherry mixture and dot the top with butter.\\n7. Roll out the second dough disk to fit the top of the pie. Use a pastry cutter or a knife to cut strips for a lattice-top crust, if desired.\\n8. Place the rolled-out top crust on top of the filling and crimp the edges to seal the pie.\\n9. Cut slits in the top crust to allow steam to escape during baking.\\n10. Bake the pie for 40-50 minutes, or until the crust is golden brown and the filling is bubbly.\\n\\nNote: This recipe assumes you're using fresh cherries. If you're using frozen cherries, thaw them first and pat dry with paper towels before using.\\n\\nI hope this helps! Do you have any specific questions about making a cherry pie?\", additional_kwargs={}, response_metadata={})]) return_messages=True memory_key='chat_history'\n"
     ]
    }
   ],
   "source": [
    "# Want to look at the memory\n",
    "print(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avery Lancaster is the Co-Founder and Chief Executive Officer (CEO) of Insurellm, an insurance technology company.\n"
     ]
    }
   ],
   "source": [
    "# Let's try a simple question\n",
    "\n",
    "#query = \"How do I change the water bottle ?\"\n",
    "query = \"Who is avery?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the  LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will bring this up in Gradio using the Chat interface -\n",
    "\n",
    "A quick and easy way to prototype a chat with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
